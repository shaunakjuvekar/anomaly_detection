{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/01 00:57:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pyspark import ml\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "spark = SparkSession.builder.appName('ml-anom').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_input = \"/data/log_data.csv\"\n",
    "\n",
    "input_df = spark.read.csv(processed_input,header='true')\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df2 = input_df.withColumn(\"is_private\",col(\"is_private\").cast(DoubleType())) \\\n",
    ".withColumn(\"is_root\",col(\"is_root\").cast(DoubleType())) \\\n",
    ".withColumn(\"is_failure\",col(\"is_failure\").cast(DoubleType())) \\\n",
    ".withColumn(\"time_since_last_failure_of_same_type\",col(\"time_since_last_failure_of_same_type\").cast(DoubleType())) \\\n",
    ".withColumn(\"failure_count_in_last_15_mins\",col(\"failure_count_in_last_15_mins\").cast(DoubleType())) \\\n",
    ".withColumn(\"failure_count_in_last_30_mins\",col(\"failure_count_in_last_30_mins\").cast(DoubleType())) \\\n",
    ".withColumn(\"failure_count_in_last_60_mins\",col(\"failure_count_in_last_60_mins\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_auth_failure\",col(\"label_auth_failure\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_break_in_attempt\",col(\"label_break_in_attempt\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_connection_closed\",col(\"label_connection_closed\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_disconnect\",col(\"label_disconnect\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_failed_password\",col(\"label_failed_password\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_invalid_user\",col(\"label_invalid_user\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_no_label\",col(\"label_no_label\").cast(DoubleType())) \\\n",
    ".withColumn(\"label_no_identification\",col(\"label_no_identification\").cast(DoubleType())) \\\n",
    ".withColumn(\"class\",col(\"class\").cast(DoubleType()))\n",
    "# df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(\"timestamp\", \"process_id\", \"username\", \"ip\", \"time_since_last_failure\")\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecCols = ['is_private', 'is_root', 'is_failure', 'time_since_last_failure_of_same_type', 'failure_count_in_last_15_mins',\n",
    "       'failure_count_in_last_30_mins', 'failure_count_in_last_60_mins','label_auth_failure', 'label_break_in_attempt',\n",
    "       'label_connection_closed', 'label_disconnect', 'label_failed_password',\n",
    "       'label_invalid_user', 'label_no_label', 'label_no_identification']\n",
    "assembler = VectorAssembler(inputCols=vecCols, outputCol=\"vectors\")\n",
    "df2 = assembler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = df2.randomSplit([0.7, 0.3], seed = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(featuresCol = 'vectors', labelCol = 'class')\n",
    "ranF = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('ml_py_model.pkl', 'wb'))\n",
    "ranF.write().overwrite().save('ml_py_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "model = RandomForestClassificationModel.load('ml_py_model.pkl')\n",
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\")\n",
    "accuracy = eval.evaluate(pred)\n",
    "print(\"Accuracy = %s\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# All columns are present. This is the data that comes from the \"data_parser\"\n",
    "cust_col = ['timestamp','process_id','username','ip','is_private','is_root','is_failure','time_since_last_failure','time_since_last_failure_of_same_type','failure_count_in_last_15_mins','failure_count_in_last_30_mins',\n",
    "            'failure_count_in_last_60_mins','label_auth_failure','label_break_in_attempt','label_connection_closed','label_disconnect','label_failed_password','label_invalid_user','label_no_label','label_no_identification','class']\n",
    "\n",
    "# First 2 lines of from the log_data.csv file, including class\n",
    "lst = [[1670673346.0,24200,'root','173.234.31.186',0,0,1,0.0,0.0,1,1,1,0,1,0,0,0,0,0,0,1], [1670673346.0,24200,'webmaster','173.234.31.186',0,0,1,0.0,0.0,2,2,2,0,0,0,0,0,1,0,0,0]]\n",
    "\n",
    "\n",
    "# Create a spark dataframe and drop unwanted values\n",
    "spark_df = spark.createDataFrame(lst, cust_col)\n",
    "spark_df = spark_df.drop(\"timestamp\", \"process_id\", \"username\", \"ip\", \"time_since_last_failure\")\n",
    "\n",
    "\n",
    "# Vectorise dataframe\n",
    "vecCols_cust = ['is_private', 'is_root', 'is_failure', 'time_since_last_failure_of_same_type', 'failure_count_in_last_15_mins', 'failure_count_in_last_30_mins', 'failure_count_in_last_60_mins','label_auth_failure', \n",
    "                'label_break_in_attempt', 'label_connection_closed', 'label_disconnect', 'label_failed_password', 'label_invalid_user', 'label_no_label', 'label_no_identification']\n",
    "assembler_cust = VectorAssembler(inputCols=vecCols_cust, outputCol=\"vectors\")\n",
    "spark_df = assembler_cust.transform(spark_df)\n",
    "\n",
    "# Import model from file and run the transform(i.e., predict) method\n",
    "model_cust = RandomForestClassificationModel.load('ml_py_model.pkl')\n",
    "pred_cust = model_cust.transform(spark_df)\n",
    "\n",
    "\n",
    "# Evaluate accuracy of custom_prediction\n",
    "eval_cust = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\")\n",
    "accuracy_cust = eval_cust.evaluate(pred_cust)\n",
    "print(\"Accuracy = %s\" % (accuracy_cust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
