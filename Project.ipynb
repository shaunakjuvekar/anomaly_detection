{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 4: PySpark Structured Streaming Using Kafka Source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b32c72eb-c581-4937-9010-f8a50738ccef;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 303ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b32c72eb-c581-4937-9010-f8a50738ccef\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/6ms)\n",
      "23/04/25 14:53:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/25 14:53:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-kafka-streaming\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"). \\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD -- dSTREAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType ,StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "\n",
    "# Define the schema for the event data\n",
    "event_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"code\", StringType()),\n",
    "    StructField(\"message\", StringType()),\n",
    "    StructField(\"ip_address\", StringType()),\n",
    "    StructField(\"label\", StringType())\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map the RDD to a new format\n",
    "# formatted_rdd = rdd.map(lambda x: (x[0], x[1], x[2], x[3], x[4], int(x[5]), int(x[6]), x[7], x[8]))\n",
    "\n",
    "# # Convert the RDD of tuples to a DataFrame with appropriate schema and column names\n",
    "# formatted_df = formatted_rdd.toDF([\"key\", \"ip_address\", \"date_time\", \"request_type\", \"request_arg\",\n",
    "#                                    \"status_code\", \"response_size\", \"referrer\", \"user_agent\"])\n",
    "\n",
    "# # Write the DataFrame to a persistent storage system\n",
    "# formatted_df.write.format(\"parquet\").mode(\"append\").save(\"hdfs://path/to/destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    rdd = batch_df.rdd.map(lambda x: (x['timestamp'], x['code'], x['message'], x['ip_address'],x['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "    .option(\"subscribe\", \"ssh\") \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")) \\\n",
    "    .selectExpr(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/25 14:53:29 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9f9c4ddc-2994-498d-b673-59b00c41f9cb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+-------+----------+-----+\n",
      "|timestamp|code|message|ip_address|label|\n",
      "+---------+----+-------+----------+-----+\n",
      "+---------+----+-------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:05|sshd[24245]|Failed password f...|112.95.230.3|failed_password|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:05|sshd[24245]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:06|sshd[24247]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:08|sshd[24247]|Failed password f...|112.95.230.3|failed_password|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:08|sshd[24247]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:08|sshd[24249]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:10|sshd[24249]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:10|sshd[24249]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:10|sshd[24251]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:12|sshd[24251]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:12|sshd[24251]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:12|sshd[24253]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:14|sshd[24253]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:14|sshd[24253]|Received disconne...|112.95.230.3|     disconnect|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:14|sshd[24255]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:16|sshd[24255]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:16|sshd[24255]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:16|sshd[24257]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:18|sshd[24257]|Failed password f...|112.95.230.3|failed_password|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:18|sshd[24257]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:19|sshd[24259]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:21|sshd[24259]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:21|sshd[24259]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:21|sshd[24261]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:23|sshd[24261]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:23|sshd[24261]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:23|sshd[24263]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:25|sshd[24263]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:25|sshd[24263]|Received disconne...|112.95.230.3|     disconnect|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:25|sshd[24265]|Invalid user utsi...|112.95.230.3|       no label|\n",
      "|Dec 10 07:28:25|sshd[24265]|input_userauth_re...|           0|   invalid_user|\n",
      "|Dec 10 07:28:25|sshd[24265]|pam_unix(sshd:aut...|           0|   auth_failure|\n",
      "|Dec 10 07:28:25|sshd[24265]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:28|sshd[24265]|Failed password f...|112.95.230.3|failed_password|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|      timestamp|       code|             message|  ip_address|          label|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "|Dec 10 07:28:28|sshd[24265]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:28|sshd[24267]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "|Dec 10 07:28:30|sshd[24267]|Failed password f...|112.95.230.3|failed_password|\n",
      "|Dec 10 07:28:30|sshd[24267]|Received disconne...|112.95.230.3|     disconnect|\n",
      "|Dec 10 07:28:31|sshd[24269]|pam_unix(sshd:aut...|112.95.230.3|   auth_failure|\n",
      "+---------------+-----------+--------------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = (df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "          .format(\"console\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "  .option(\"topic\", \"topic_test\") \\\n",
    "  .trigger(processingTime = '5 seconds')\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"ip_address\", StringType()),\n",
    "    StructField(\"date_time\", StringType()),\n",
    "    StructField(\"request_type\", StringType()),\n",
    "    StructField(\"request_arg\", StringType()),\n",
    "    StructField(\"status_code\", StringType()),\n",
    "    StructField(\"response_size\", StringType()),\n",
    "    StructField(\"referrer\", StringType()),\n",
    "    StructField(\"user_agent\", StringType())\n",
    "])\n",
    "\n",
    "# Parse the events from JSON format\n",
    "df_parsed = (df_streamed_kv\n",
    "           # Sets schema for event data\n",
    "           .withColumn(\"value\", from_json(\"value\", event_schema))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = (df_parsed.select(\n",
    "    col(\"key\").alias(\"event_key\")\n",
    "    ,col(\"topic\").alias(\"event_topic\")\n",
    "    ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "    ,col(\"value.ip_address\").alias(\"ip_address\")\n",
    "    ,col(\"value.date_time\").alias(\"date_time\")\n",
    "    ,col(\"value.request_type\").alias(\"request_type\")\n",
    "    ,col(\"value.request_arg\").alias(\"request_arg\")\n",
    "    ,col(\"value.status_code\").alias(\"status_code\")\n",
    "    ,col(\"value.response_size\").cast(IntegerType()).alias(\"response_size\")\n",
    "    ,col(\"value.referrer\").alias(\"referrer\")\n",
    "    ,col(\"value.user_agent\").alias(\"user_agent\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q2.2:** All your code for 2.2 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 2.2\n",
    "query = (df_formatted\n",
    " .writeStream \\\n",
    " .outputMode(\"update\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\") \\\n",
    "  .option(\"topic\", \"topic_test\") \\\n",
    "  .trigger(processingTime = '5 seconds')\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name of active streams (This may be useful during debugging)\n",
    "for s in spark.streams.active:\n",
    "    print(f\"ID:{s.id} | NAME:{s.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==== Q3 ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.1:** All your code for 3.1 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = df_formatted.groupBy(\"event_topic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (cnt \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"count\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.2:** All your code for 3.2 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "windowedCounts = df_formatted \\\n",
    "    .groupBy(\n",
    "        window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"),\n",
    "        df_formatted.request_type) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = (windowedCounts \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"windowed_count\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from windowed_count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.3:** All your code for 3.3 should be in the following cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = df_formatted.groupBy(\n",
    "        window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"),\n",
    "        df_formatted.request_type) \\\n",
    "    .agg(\n",
    "      avg(\"response_size\").alias(\"response_size_average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = (avg \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"response_size_average\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from response_size_average\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q3.4:** All your code for 3.4 should be in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "std = df_formatted \\\n",
    "    .groupBy( window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"),\n",
    "    df_formatted.request_type) \\\n",
    "    .agg(avg(\"response_size\").alias(\"Average\"),\n",
    "    stddev(\"response_size\").alias(\"SDev\"), \n",
    "    count(\"response_size\").alias(\"Count\"),\n",
    "    collect_list(\"response_size\").alias(\"List\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = (std \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"response_size_std\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from response_size_std\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = std.select(\"window\",\"request_type\",explode(\"List\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = (exp \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"exp_table\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from exp_table\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "expr1 = df_formatted \\\n",
    "    .groupBy( window(df_formatted.event_timestamp, \"10 seconds\", \"10 seconds\"),\n",
    "    df_formatted.request_type) \\\n",
    "    .agg(avg(\"response_size\").alias(\"Average\"),\n",
    "    stddev(\"response_size\").alias(\"SDev\"), \n",
    "    collect_list(\"response_size\").alias(\"List\"))\\\n",
    "    .select(\"window\",\"Average\",\"SDev\",explode(\"List\").alias(\"Exploded_list\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = expr1.filter(expr(\"Exploded_list > Average +SDev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = (filter1 \\\n",
    " .writeStream\\\n",
    " .format(\"memory\") \\\n",
    " .outputMode(\"update\") \\\n",
    " .trigger(processingTime = '5 seconds')\\\n",
    " .queryName(\"expr_table\")\\\n",
    " .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"Select * from expr_table\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
